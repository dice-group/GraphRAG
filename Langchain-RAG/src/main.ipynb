{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import openai\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Configure API settings\n",
    "os.environ[\"TENTRIS_BASE_URL_EMBEDDINGS\"] = os.getenv(\"TENTRIS_BASE_URL_EMBEDDINGS\")\n",
    "os.environ[\"TENTRIS_API_KEY\"] = os.getenv(\"TENTRIS_API_KEY\")\n",
    "os.environ[\"TENTRIS_BASE_URL_CHAT\"] = os.getenv(\"TENTRIS_BASE_URL_CHAT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. Embedding Functions\n",
    "\n",
    "# %%\n",
    "def embed_documents(texts):\n",
    "    client = openai.OpenAI(\n",
    "        base_url=os.getenv(\"TENTRIS_BASE_URL_EMBEDDINGS\"),\n",
    "        api_key=os.getenv(\"TENTRIS_API_KEY\"),\n",
    "        timeout=60\n",
    "    )\n",
    "    responses = client.embeddings.create(input=texts, model=\"tentris\")\n",
    "    return [data.embedding for data in responses.data]\n",
    "\n",
    "def embed_query(text):\n",
    "    client = openai.OpenAI(\n",
    "        base_url=os.getenv(\"TENTRIS_BASE_URL_EMBEDDINGS\"),\n",
    "        api_key=os.getenv(\"TENTRIS_API_KEY\"),\n",
    "        timeout=60\n",
    "    )\n",
    "    response = client.embeddings.create(input=[text], model=\"tentris\")\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "First document sample:\n",
      "Simon Bin is a person who was affiliated with DICE Research. \n",
      "He has worked on the DAIKIRI project and is listed as an alumnus. \n",
      "Simon's email is sbin@informatik.uni-leipzig.de, and his chat handle is...\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"../data/speech.txt\")  # Update path as needed\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(\"First document sample:\")\n",
    "print(documents[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using first 2 chunks only\n",
      "\n",
      "Chunk 1:\n",
      "Simon Bin is a person who was affiliated with DICE Research. \n",
      "He has worked on the DAIKIRI project and is listed as an alumnus. \n",
      "Simon's email is sbin@informatik.uni-leipzig.de, and his chat handle is @sbin:chat.dice-research.org. \n",
      "His phone number is +49-341-22903736, and his office is located in Leipzig.\n",
      "Simon Bin is a person who was affiliated with DICE Research. He has worked on the DAIKIRI project and is listed as an alumnus. Simon's email is sbin@informatik.uni-leipzig.de, and his chat handle is @sbin:chat.dice-research.org. His phone number is +49-341-22903736, and his office is located in Leipzig.\n",
      "Jan Reineke is an alumnus of DICE Research and holds the position of Person. His office is located at TP6.3.307. His email address is jan.reineke@uni-paderborn.de, and his chat handle is @jarei:chat.dice-research.org. His phone number is +49-525-16-05190, and his photo is available at reineke.jpg.\n",
      "\n",
      "Chunk 2:\n",
      "RenÃƒÂ© Speck is a research staff member at DICE Research. He is located in office TP6.3.102 and can be reached via chat at @rspeck:chat.dice-research.org. RenÃƒÂ© is involved in several projects, including GEISER, HOBBIT, Qamel, SAIM, BioASQ, DIESEL, SCMS, RAKI, and Gerbil. His email address is rene.speck@uni-paderborn.de, and his photo is available as speck.jpg.\n",
      "Axel-Cyrille Ngonga Ngomo, a professor and doctor, is the head of DICE Research. He is involved in various projects including OpenTrafficDetection, QAMEL, SAIL, OPAL, DIESEL, SAKE, LinkingLOD, SCMS, SAGE, BDE, GEISER, SLIPO, HOBBIT, embeddings.cc, ENEXA, and GeoKnow. He can be reached via email at axel.ngonga@upb.de, by phone at +49-525-16-03342, or by fax at +49-525-16-03436. His office is located in TP6.3.106, and he is available for chat at @ngonga:chat.dice-research.org. His photo is available as ngonga.jpg.\n",
      "Idress Tahir Mugdal is an alumnus of DICE. Their office, phone, and fax numbers are not specified. They have a photo named \"mugdal.jpg\" and their email address is available....\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "split_docs = text_splitter.split_documents(documents)[:2]  # <-- Only keep first 2 chunks\n",
    "\n",
    "print(f\"Using first {len(split_docs)} chunks only\")\n",
    "print(\"\\nChunk 1:\")\n",
    "print(split_docs[0].page_content[:])\n",
    "print(\"\\nChunk 2:\")\n",
    "print(split_docs[1].page_content[:] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store ready!\n"
     ]
    }
   ],
   "source": [
    "index_path = \"faiss_index\"\n",
    "\n",
    "if os.path.exists(index_path):\n",
    "    print(\"Loading existing index...\")\n",
    "    db = FAISS.load_local(\n",
    "        index_path,\n",
    "        embed_query,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Creating new index...\")\n",
    "    # Get text content for embedding\n",
    "    texts = [doc.page_content for doc in split_docs]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = embed_documents(texts)\n",
    "    \n",
    "    # Create FAISS index\n",
    "    db = FAISS.from_embeddings(\n",
    "        text_embeddings=list(zip(texts, embeddings)),\n",
    "        embedding=embed_query\n",
    "    )\n",
    "    db.save_local(index_path)\n",
    "\n",
    "print(\"Vector store ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 relevant chunks:\n",
      "\n",
      "Chunk 1 (874 characters):\n",
      "Person Profile: Farshad Afshari\n",
      "schema1:namePrefix: \n",
      "schema1:role: dice:Infrastructure\n",
      "schema1:phone: tel:\n",
      "schema1:fax: tel:\n",
      "schema1:email: ns1:uni-paderborn.de\n",
      "schema1:chat: @afshari:chat.dice-resear...\n",
      "\n",
      "Chunk 2 (361 characters):\n",
      "Person Profile: Ajay Kumar\n",
      "schema1:project: dice:ClimatebOWL\n",
      "schema1:role: dice:StudentResearcher\n",
      "schema1:photo: ajayKumar.jpg\n",
      "schema1:content:  I am a computer science student interested in integrati...\n",
      "\n",
      "Chunk 3 (936 characters):\n",
      "Additional Information:\n",
      "# Bio\n",
      "\n",
      "Kunal is a  Masters Student (Computer Science) at University of Bonn and works as a Research Assistant at AKSW in Leipzig. Before joining AKSW, Kunal has completed Bac...\n"
     ]
    }
   ],
   "source": [
    "# ## 5. Similarity Search Debugging\n",
    "\n",
    "# %%\n",
    "# Test question\n",
    "test_question = \"What is the main theme of the document?\"\n",
    "\n",
    "# Perform similarity search\n",
    "similar_docs = db.similarity_search(test_question, k=3)\n",
    "\n",
    "print(f\"Found {len(similar_docs)} relevant chunks:\")\n",
    "for i, doc in enumerate(similar_docs, 1):\n",
    "    print(f\"\\nChunk {i} ({len(doc.page_content)} characters):\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed prompt:\n",
      "Context: Person Profile: Farshad Afshari\n",
      "schema1:namePrefix: \n",
      "schema1:role: dice:Infrastructure\n",
      "schema1:phone: tel:\n",
      "schema1:fax: tel:\n",
      "schema1:email: ns1:uni-paderborn.de\n",
      "schema1:chat: @afshari:chat.dice-research.org\n",
      "schema1:office: FU.201.4\n",
      "schema1:photo: afshari.jpg\n",
      "schema1:project: dice:FROCKG\n",
      "schema1:project: dice:NEBULA\n",
      "schema1:project: dice:ENEXA\n",
      "schema1:content: \n",
      "  <p>\n",
      "  <b>Linkedin </b><a href=\"https://www.linkedin.com/in/afsharifarshad/\"> &ensp;Farshad Afshari </a>\n",
      "  </p>\n",
      "  <hr/>\n",
      "  <p>\n",
      "  <b>Personal website </b><a href=\"https://farshadafshari.com/\"> &ensp;Farshad Afshari website </a>\n",
      "  </p>\n",
      "  \n",
      "\n",
      "Additional Information:\n",
      "\n",
      "  <p>\n",
      "  <b>Linkedin </b><a href=\"https://www.linkedin.com/in/afsharifarshad/\"> &ensp;Farshad Afshari </a>\n",
      "  </p>\n",
      "  <hr/>\n",
      "  <p>\n",
      "  <b>Personal website </b><a href=\"https://farshadafshari.com/\"> &ensp;Farshad Afshari website </a>\n",
      "  </p>\n",
      "\n",
      "Person Profile: Ajay Kumar\n",
      "schema1:project: dice:ClimatebOWL\n",
      "schema1:role: dice:StudentResearcher\n",
      "schema1:photo: ajayKumar.jpg\n",
      "schema1:content:  I am a computer science student interested in integration and automation.\n",
      "schema1:email: ns1:campus.uni-paderborn.de\n",
      "\n",
      "Additional Information:\n",
      " I am a computer science student interested in integration and automation.\n",
      "\n",
      "Additional Information:\n",
      "# Bio\n",
      "\n",
      "Kunal is a  Masters Student (Computer Science) at University of Bonn and works as a Research Assistant at AKSW in Leipzig. Before joining AKSW, Kunal has completed Bachelors in Information and Communication Technology f...\n"
     ]
    }
   ],
   "source": [
    "# ## 6. Prompt Construction\n",
    "\n",
    "# %%\n",
    "# Build context\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in similar_docs])\n",
    "prompt = f\"\"\"Context: {context}\n",
    "\n",
    "Question: {test_question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"Constructed prompt:\")\n",
    "print(prompt[:1500] + \"...\" if len(prompt) > 1500 else prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final answer:\n",
      "The main theme of the document is the profiles of Farshad Afshari and Ajay Kumar, detailing their roles, contact information, projects, and areas of interest within the context of the DICE research group at the University of Paderborn. Additionally, there is a brief bio provided for Kunal, who is a Masters Student at the University of Bonn and a Research Assistant at AKSW in Leipzig.\n"
     ]
    }
   ],
   "source": [
    "# ## 7. Query Execution\n",
    "\n",
    "# %%\n",
    "def get_chat_response(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        base_url=os.getenv(\"TENTRIS_BASE_URL_CHAT\"),\n",
    "        api_key=os.getenv(\"TENTRIS_API_KEY\")\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"tentris\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# %%\n",
    "# Get final answer\n",
    "answer = get_chat_response(prompt)\n",
    "print(\"Final answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdflib in c:\\users\\kunjan shah\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (7.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kunjan shah\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: html5lib-modern<2.0,>=1.2 in c:\\users\\kunjan shah\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rdflib) (1.2)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in c:\\users\\kunjan shah\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rdflib) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kunjan shah\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Loading 104 RDF files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing RDF Files: 100%|██████████| 104/104 [00:02<00:00, 36.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1165 triples from RDF files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Triples: 100%|██████████| 1165/1165 [00:00<00:00, 129577.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 1165 textual triples.\n",
      "✅ Processed RDF data saved to rdf_extracted.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "🛠️ How It Works\n",
    "Loads RDF (.ttl) files from a specified folder.\n",
    "Parses the RDF graph using rdflib.Graph().\n",
    "Extracts subject-predicate-object triples.\n",
    "Formats entities:\n",
    "URIs → Shortened (last part of the URL)\n",
    "Literals → Kept as-is\n",
    "Saves processed text in a .txt file for RAG applications.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%pip install rdflib tqdm\n",
    "\n",
    "import os\n",
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, FOAF, OWL, XSD\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RDFParser:\n",
    "    def __init__(self, directory: str, output_file: str):\n",
    "        \"\"\"\n",
    "        Initializes the RDF parser.\n",
    "        :param directory: Path to folder containing RDF/Turtle files.\n",
    "        :param output_file: Path to save extracted text.\n",
    "        \"\"\"\n",
    "        self.directory = directory\n",
    "        self.output_file = output_file\n",
    "        self.graph = rdflib.Graph()\n",
    "\n",
    "    def load_rdf_files(self):\n",
    "        \"\"\"Loads all .ttl files from the given directory into an RDF graph.\"\"\"\n",
    "        ttl_files = [f for f in os.listdir(self.directory) if f.endswith(\".ttl\")]\n",
    "        print(f\"Loading {len(ttl_files)} RDF files...\")\n",
    "\n",
    "        for file in tqdm(ttl_files, desc=\"Parsing RDF Files\"):\n",
    "            file_path = os.path.join(self.directory, file)\n",
    "            self.graph.parse(file_path, format=\"turtle\")\n",
    "\n",
    "        print(f\"✅ Loaded {len(self.graph)} triples from RDF files.\")\n",
    "\n",
    "    def extract_triples(self):\n",
    "        \"\"\"Extracts triples and converts them into readable text format.\"\"\"\n",
    "        extracted_text = []\n",
    "        for s, p, o in tqdm(self.graph, desc=\"Extracting Triples\"):\n",
    "            s_text = self.format_entity(s)\n",
    "            p_text = self.format_entity(p)\n",
    "            o_text = self.format_entity(o)\n",
    "\n",
    "            extracted_text.append(f\"{s_text} {p_text} {o_text}.\")\n",
    "\n",
    "        print(f\"✅ Extracted {len(extracted_text)} textual triples.\")\n",
    "        return extracted_text\n",
    "\n",
    "    def format_entity(self, entity):\n",
    "        \"\"\"Formats an RDF entity: shortens URIs or keeps literals.\"\"\"\n",
    "        if isinstance(entity, rdflib.URIRef):\n",
    "            return entity.split(\"/\")[-1]  # Get the last part of the URI\n",
    "        elif isinstance(entity, rdflib.Literal):\n",
    "            return str(entity)  # Keep literals as-is\n",
    "        return entity.n3()\n",
    "\n",
    "    def save_text(self, text_data):\n",
    "        \"\"\"Saves extracted text to a file.\"\"\"\n",
    "        with open(self.output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(text_data))\n",
    "        print(f\"✅ Processed RDF data saved to {self.output_file}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs the entire pipeline: Load, Extract, Save.\"\"\"\n",
    "        self.load_rdf_files()\n",
    "        text_data = self.extract_triples()\n",
    "        self.save_text(text_data)\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    rdf_parser = RDFParser(directory=\"../dice-website/data/people/\", output_file=\"rdf_extracted.txt\")\n",
    "    rdf_parser.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
